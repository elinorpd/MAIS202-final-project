# -*- coding: utf-8 -*-
"""MAIS Final Project-Elinor Poole-Dayan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wJbxyTvkjratU3H7LPFQ1rs3fzp5fgM6

**Troll dataset from kaggle: https://www.kaggle.com/fivethirtyeight/russian-troll-tweets**
Columns:

`content` | The text of the tweet

`language` | The language of the tweet

`account_type` | Specific account theme, as coded by Linvill and Warren
  -> types are LeftTroll, RightTroll, Commercial, NewsFeed, Fearmonger, HashtagGamer, and Unknown.




---




**Nontroll dataset from Kaggle: https://www.kaggle.com/speckledpingu/RawTwitterFeeds**

Columns:

`text` | The text of the tweet

# Downloading Data
"""

!pip install -q kaggle
from google.colab import files
uploaded = files.upload() # upload the kaggle.json file

!mkdir ~/.kaggle
!mv kaggle.json ~/.kaggle/

!kaggle datasets download -d fivethirtyeight/russian-troll-tweets
!kaggle datasets download -d speckledpingu/RawTwitterFeeds

!unzip russian-troll-tweets.zip
!unzip RawTwitterFeeds.zip

import pandas as pd
import csv
import random
import nltk
from nltk.stem.snowball import SnowballStemmer
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
import matplotlib
import matplotlib.pyplot as plt
import pickle
from sklearn.metrics import precision_recall_fscore_support, log_loss
from sklearn.metrics import precision_recall_curve

"""# Data Preprocessing"""

#create all the dataframes
df1 = pd.read_csv('IRAhandle_tweets_1.csv')
df2 = pd.read_csv('IRAhandle_tweets_2.csv')
df3 = pd.read_csv('IRAhandle_tweets_3.csv')
df4 = pd.read_csv('IRAhandle_tweets_4.csv')
df5 = pd.read_csv('IRAhandle_tweets_5.csv')
df6 = pd.read_csv('IRAhandle_tweets_6.csv')
df7 = pd.read_csv('IRAhandle_tweets_7.csv')
df8 = pd.read_csv('IRAhandle_tweets_8.csv')
df9 = pd.read_csv('IRAhandle_tweets_9.csv')
df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9])
df_nontroll = pd.read_csv('AllTweets.csv')

#add nontroll label all nontroll tweets in df
nontroll_cats = np.full(len(df_nontroll), fill_value="NonTroll")
df_nontroll['account_category'] = nontroll_cats

"""Removing unnecessary columns and merging the two datasets"""

#TROLL DATA
#removing columns that we don't need
df_troll_all = df[['content','language', 'account_category']]

#keep only the tweets in English and remove the language category column
df_troll = df_troll_all[df_troll_all['language'].map(lambda x: str(x)=="English")]
#drop NonEnglish and Unknown categories
df_troll = df_troll_all[df_troll_all['account_category'].map(lambda x: (str(x)!="NonEnglish" and str(x) != "Unknown"))][["content", "account_category"]]

#NONTROLL DATA
#removing columns that we don't need
df_nontroll = df_nontroll[['text', 'account_category']]
#relabeling the first column to be able to merge properly
df_nontroll = df_nontroll.rename(columns={"text": "content"})

#merge all data
df_all = pd.concat([df_troll, df_nontroll], ignore_index=True, axis=0)
#drop NaNs
df_all.dropna(inplace=True)

"""How many of each category do we have in the complete dataset?"""

print(df_all.account_category.value_counts())
print("---"*20)
print("Nontroll shape:", df_nontroll.shape)
print("Troll shape:", df_troll.shape)
print("All data shape:", df_all.shape)

"""Cleaning data function"""

#create stemmer
stemmer = SnowballStemmer("english")

stop_words = ['', 'i', 'im', 'me', 'my', 'myself', 'we', 'our', 'ourselv',
              'you', 'your', 'youv', 'youll', 'youd', 'yourself', 'yourselv',
              'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'herself', 
              'it', 'itself', 'they', 'them', 'their', 'themselv', 'what', 
              'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 
              'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
              'has', 'had', 'do', 'doe', 'did', 'a', 'an', 'the', 'and', 'but',
              'if', 'or', 'becaus', 'as', 'until', 'while', 'of', 'at', 'by',
              'for', 'with', 'about', 'between', 'into', 'through', 'dure',
              'befor', 'after', 'abov', 'below', 'to', 'from', 'in', 'out', 
              'on', 'off', 'over', 'under', 'further', 'then', 'onc', 'here',
              'there', 'when', 'where', 'whi', 'how', 'all', 'ani', 'both', 
              'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 
              'nor', 'not', 'onli', 'own', 'same', 'so', 'than', 'too', 'veri',
              's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 
              'shouldv', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain',
              'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 
              'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 
              'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt',
              'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn',
              'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt']

#clean function
def clean(content):
  content = str(content)
  words = content.split()
  list_of_words = []
  for i, word in enumerate(words):
    #make lowercase
    word = word.lower()
    #remove the links
    if ("http" not in word):
    #keep the tags and mentions but get rid of punctuation
      for char in word:
        if (char != ' ') and (char != '@') and (char != '#') and (not char.isalpha()) and (not char.isdecimal()):
          word = word.replace(char, '')
      #stem regular words
      if ('@' not in word) and ('#' not in word):
        word = stemmer.stem(word)
      #remove stop words
      if word not in stop_words:
        list_of_words.append(word)
  return list_of_words #list of words - is essentially tokenized and stemmed

"""Splitting, cleaning, and vectorizing the data"""

#splitting data
X_train, X_test, y_train, y_test = train_test_split(list(np.array(df_all['content'])), list(np.array(df_all['account_category'])), test_size=0.25, random_state=1)
#initialize vectorizer based on X_train
#vectorizer = CountVectorizer(input=X_train, analyzer=clean, min_df=10, binary=True, ngram_range=(1,2)) #bigram made model a bit worse

vectorizer = CountVectorizer(analyzer=clean, min_df=5, binary=True)
vectorizer = vectorizer.fit(X_train)

#saving it in pickle
filename = 'vectorizer.sav'
pickle.dump(vectorizer, open(filename, 'wb'))
#uncomment if you want to download
#files.download(filename)

#vectorize X_train
X_train_vectorized = vectorizer.transform(X_train)
#vectorize X_test
X_test_vectorized = vectorizer.transform(X_test)

"""Label Encoding of categories"""

le = preprocessing.LabelEncoder()
le.fit(np.array(df_all['account_category']))
y_train_encoded = le.transform(y_train)
y_test_encoded = le.transform(y_test)
print("The categories are:", le.classes_)

"""# Creating the model: Stochastic Gradient Descent

Training and fitting the model, saving and downloading the weights
"""

clf = SGDClassifier(class_weight='balanced', loss = 'modified_huber')
clf.fit(X_train_vectorized, y_train_encoded)

#saving it in pickle
filename = 'finalized_model.sav'
pickle.dump(clf, open(filename, 'wb'))
#uncomment if you want to download
#files.download(filename)

"""Accessing the model later"""

#accessing it later 
#loaded_model = pickle.load(open(filename, 'rb'))
#result = loaded_model.score(X_test_vectorized, y_test_encoded)
#print(result)
#clf.get_params

"""Predicting"""

#predict values on test set
y_pred = clf.predict(X_test_vectorized)
y_pred_prob = clf.predict_proba(X_test_vectorized)

"""# Results

Creating the confusion matrix and printing accuracy, precision recall, and log loss.
"""

#create confusion matrix
cf = confusion_matrix(y_test_encoded, y_pred)
#print mean accuracy of the model
print("Train mean accuracy", str((clf.score(X_train_vectorized, y_train_encoded)*100)) +'%')
print("Test mean accuracy", str((clf.score(X_test_vectorized, y_test_encoded)*100)) +'%')
print("Precision recall:", precision_recall_fscore_support(y_test_encoded, y_pred, average=None,labels = [0,1,2,3,4,5,6]))
print("Log loss:", log_loss(y_test_encoded, y_pred_prob, labels = [0,1,2,3,4,5,6]))

"""Plotting the confusion matrix"""

fig, ax = plt.subplots()
im = ax.imshow(cf, cmap='Blues')
cbar = ax.figure.colorbar(im, ax=ax)
cbar.ax.set_ylabel(ylabel='', rotation=-90, va="bottom")
# We want to show all ticks...
ax.set_xticks(np.arange(len(le.classes_)))
ax.set_yticks(np.arange(len(le.classes_)))
# ... and label them with the respective list entries
ax.set_xticklabels(le.classes_)
ax.set_yticklabels(le.classes_)

# Rotate the tick labels and set their alignment.
plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

ax.set_title("Confusion Matrix")
plt.show()

"""# Predicting

Function to print classification and certainty
"""

def result(tweet):
  #get the prediction
  num_cat = clf.predict(vectorizer.transform([tweet]))
  pred_cat = le.inverse_transform(num_cat)[0]
  #get confidence score matrix
  conf_score = clf.decision_function(vectorizer.transform([tweet]))[0]
  #find certainty of the predicted category
  certainty = round((np.amax(np.exp(conf_score)/sum(np.exp(conf_score)))*100),2)
  print("This tweet is a", pred_cat, "tweet with", str(certainty) + "% certainty.")

"""Some tests:"""

result('black lives matter #democrat #blacklivesmatter #blm')
result('donald trump is the best president yay russia')
result('recent school shooting killed 70 children')
result('eating healthy is the best way to lose weight #detox')
result('love yourself #mentalhealth #selflove')
result('my dog is so cute #blacklivesmatter #pets')
result('my dog is so cute #pets')
result('my dog is so cute #maga #pets')
result('breaking news, @CNN the US bombed Afghanistan and now world war 3 started #war')